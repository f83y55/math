<!DOCTYPE html>
<html lang="fr">

<head>
    <title></title>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <script src="../../../../s.js"></script>
</head>

<body onload="body()">
    <main>
        <h1>Probabilités 2</h1>
        <h2>Variables aléatoires : compléments</h2>
        <div class="row">
            <div class="col" style="width:50%;">
                <div class="defi"><strong>Rappels : </strong>Lorsque \(X\) est une variable aléatoire :<br>
                    <ul>
                        <li><strong>Variance : </strong><br>\(\begin{array}{ccl}&&\textrm{Var}(X) \\ &=&
                            \mathbb{E}\left(X-\mathbb{E}(X)\right)^2\\
                            &=& \mathbb{E}(X^2)-\left(\mathbb{E}(X)\right)^2\\
                            \end{array}\)</li>
                        <li><strong>Écart-type : </strong><br>\(\sigma(X) = \sqrt{\textrm{Var}(X)}\)
                        </li>
                    </ul>
                </div>
            </div>
            <div class="col">
                <div class="prop">Si a et b sont deux constantes, et \(X\) et \(Y\) deux variables aléatoires :
                    <ul>
                        <li>\(\mathbb{E}(aX+b)=a\mathbb{E}(X)+b\)</li>
                        <li>\(\textrm{Var}(aX+b)=a^2\textrm{Var}(X)\)</li>
                        <li>\(X\leqslant Y \Rightarrow \mathbb{E}(X)\leqslant\mathbb{E}(Y)\)</li>
                    </ul>
                </div>
                <div class="exo">
                    Montrer que si \(X\) est une variable aléatoire d'espérance \(\mu\) et d'écart-type \(\sigma\),
                    alors \(Z =
                    \dfrac{X-\mu}{\sigma} \) est une variable aléatoire d'espérance 0 (on dit centée) et d'écart-type 1
                    (on dit
                    réduite).
                </div>
            </div>

        </div>
        <div class="defi"> \( X_1 , \ldots , X_n \) sont \(n\) variables aléatoires.<br>
            On dit que \( X_1 , \ldots , X_n \) sont <strong>indépendantes</strong> lorsque pour tous \(1\leqslant i
            &lt;
            j
            \leqslant n\) entiers et pour tous les intervalles réels \(A, B\), on a :
            \(\textrm{P}\left(X_i\in A\textrm{ et }X_j\in B\right)
            = \textrm{P}\left(X_i\in A\right)\times \textrm{P}\left(X_j\in B\right)
            \).
        </div>
        <div class="rema">
            Pour des variables aléatoires discrètes, on peut utiliser : <br>
            \( X_1 , \ldots , X_n \) sont
            <strong>indépendantes</strong> lorsque pour tous \(1\leqslant i
            &lt;
            j
            \leqslant n\) entiers et pour tous les réels \(a, b\), on a :
            \(\textrm{P}\left(X_i=a\textrm{ et }X_j=b\right)
            = \textrm{P}\left(X_i=a\right)\times \textrm{P}\left(X_j=b\right)
            \).
        </div>

        <div class="exo">
            On note \(X\) une variable aléatoire de 3 valeurs {-1 ; 0 ; 1} équiprobables. On note \(Y\) la variable
            aléatoire qui vaut 1 lorsque \(X=0\) et 0 sinon.<br>
            Montrer que \(X\) et \(Y\) ne sont pas indépendantes.
        </div>

        <div class="prop"> \( X_1 , \ldots , X_n \) sont \(n\) variables aléatoires.<br>
            On note leur somme \(S_n = X_1 + \cdots + X_n\)
            et leur moyenne \(M_n = \frac{1}{n}\left(X_1 + \cdots + X_n\right)\)
            <ul>
                <li>
                    <span class="surl">
                        \(\mathbb{E}(S_n) = \mathbb{E}(X_1)+\cdots+\mathbb{E}(X_n)\)
                    </span>
                    &nbsp;&nbsp;et&nbsp;&nbsp;
                    \(\mathbb{E}(M_n) = \frac{1}{n}\left(\mathbb{E}(X_1)+\cdots+\mathbb{E}(X_n)\right)\)
                </li>
                <li><strong>Si les variables aléatoires sont indépendantes (non-corrélées est suffisant)</strong> :<br>
                    <span class="surl">
                        \(\textrm{Var}(S_n) = \textrm{Var}(X_1)+\cdots+\textrm{Var}(X_n)\)
                    </span>
                    &nbsp;&nbsp;et&nbsp;&nbsp;
                    \(\textrm{Var}(M_n) = \frac{1}{n^2}\left(\textrm{Var}(X_1)+\cdots+\textrm{Var}(X_n)\right)\)
                </li>
                <li><strong>Si les variables aléatoires sont indépendantes (non-corrélées est suffisant) et ont même
                        espérance \(\mu\) et même variance \(v\) (donc même écart-type \(\sigma\))</strong> :<br>
                    \(\mathbb{E}\left(S_n\right)=n\mu\) et \(\mathbb{E}\left(M_n\right)=\mu\)
                    &nbsp;&nbsp;;&nbsp;&nbsp;
                    \(\textrm{Var}\left(S_n\right)=nv\)
                    et \(\textrm{Var}\left(M_n\right)=\frac{1}{n}v\)
                    &nbsp;&nbsp;;&nbsp;&nbsp;
                    \(\sigma\left(S_n\right)=\sigma\sqrt{n}\)
                    \(\sigma\left(M_n\right)=\dfrac{\sigma}{\sqrt{n}}\)
                </li>

            </ul>
            </li>
            </ul>
        </div>
        <div class="exo">
            Déduire les propriétés précédentes à partir des deux premières égalités donnant l'espérance et la variance
            de \(S_n\).
        </div>
        <div class="exo">Montrer que si les \( X_1 , \ldots , X_n \) sont \(n\) variables aléatoires indépendantes
            suivant une loi de Bernoulli de probabilité \(p\) (\(x=1\) avec une probabilité \(p\) et \(x=0\) avec une
            probabilité \(1-p\)), alors \(S_n\) suit une loi binomiale de paramètres \(n\) et \(p\).
            En déduire que \(\mathbb{E}(S_n)=np\) et que \(\sigma(S_n)=\sqrt{np(1-p)}\).
        </div>

        <div class="exo">
            \(X\) et \(Y\) sont deux variables aléatoires indépendantes d'espérance respectivement 15 et 20 et de
            variance 10 et 5.
            <ol>
                <li>Déterminer les espérances, variances et écart-types de :
                    <ul class="row">
                        <li>\(Z_1=4X\)</li>
                        <li>\(Z_2=-10Y\)</li>
                        <li>\(Z_3=X+Y\)</li>
                        <li>\(Z_4=4X-10Y\)</li>
                    </ul>
                </li>
                <li>Si \(X\) et \(Y\) mesurent la durée de deux tâches successives indépendantes composant un projet,
                    quelle variable aléatoire \(Z_n\) mesure la durée du projet ?</li>
        </div>

        <h2 class="section">Échantillons, loi des grands nombres</h2>
        <div class="defi">
            <ul>
                <li> Une <strong>population</strong> de taille N est modélisée par un ensemble de variables aléatoires
                    \(\left\{Y_1,\dots,Y_N\right\}\), qui représentent le caractère mesuré sur chaque individu.
                    <br> On
                    suppose, sauf cas particulier, que ces variables aléatoires suivent la même loi et sont
                    indépendantes.
                </li>
                <li> On sélectionne n individus, qui forment un <b>échantillon</b> de taille n&lt;N ; on a donc
                    un
                    ensemble de variables aléatoires \(\left\{X_1,\ldots,X_n\right\}\), tel que, par exemple,
                    \(X_1=Y_3\), \(X_2=Y_{14}\), ... </li>
                <li> Une <b>réalisation</b> de cette échantillon consiste à donner à chaque variable aléatoire
                    de
                    l'échantillon une valeur réelle, selon la loi suivie par cette variable aléatoire : on
                    mesure le
                    caractère étudié sur l'échantillon. <br>
                    Une réalisation d'un n-échantillon se traduit donc par l'obtention de n valeurs :
                    \(x_1,\ldots,x_n\)
                    (notées en minuscule).</li>
            </ul>
        </div>

        <div class="prop"><strong>Inégalité de Bienaymé-Tchebychev</strong><br>
            \(X\) est une variable aléatoire d'espérance \(\mu\) et de variance \(v\).<br>
            Pour tout réel strictement positif \(\delta\), on a :
            \(P\left(\left|X-\mu\right|\geqslant\delta\right) \leqslant \dfrac{\textrm{Var(X)}}{\delta^2}\)
        </div>

        <div class="rema">
            \(\left|X-\mu\right|\geqslant\delta\) équivaut à \(X\leqslant\mu-\delta\) ou \(X\geqslant\mu+\delta\), c'est
            à dire \(X\) est à l'extérieur de l'intervalle \(\left]\mu-\delta;\mu+\delta\right[\).
        </div>

        <div class="exo"><strong>Démonstration : </strong><br>
            On définit la variable aléatoire
            \(Z = \left|\begin{array}{l}
            \delta^2 \textrm{ si } \left(X-\mu\right)^2\geqslant \delta^2\\
            0 \textrm{ sinon}\\
            \end{array}\right.
            \)
            <ol style="margin: 0;padding: 0;">
                <li>
                    En écrivant la loi de \(Z\), montrer que
                    \(\mathbb{E}(Z) = \delta^2 P\left(\left|X-\mu\right|\geqslant\delta\right)\)
                </li>
                <li>
                    Justifier que \(0\leqslant Z\leqslant \left(X-\mu\right)^2 \).
                    En déduire que \(0\leqslant \mathbb{E}(Z)\leqslant \mathbb{E}\left(\left(X-\mu\right)^2\right) =
                    \textrm{Var}(X)\)
                    et conclure.
                </li>
            </ol>

        </div>

        <div class="prop"><strong>Inégalité de concentration</strong><br>
            \(M_n\) est une variable aléatoire moyenne d'un échantillon de taille \(n\) de variables aléatoires
            indépendantes d'espérance \(\mu\) et de variance \(v\).<br>
            Pour tout réel strictement positif \(\delta\), on a :
            \(P\left(\left|M_n-\mu\right|\geqslant\delta\right) \leqslant \dfrac{v}{n\delta^2}\)
        </div>
        <div class="exo"><strong>Démonstration : </strong><br>
            Appliquer l'inégalité de Bienaymé-Tchebychev à \(M_n\) (quelle est sa variance ?).<br>
            Que se passe-t-il lorsque \(n\) est très grand ?
        </div>

        <div class="theo"><strong>Loi faible des grands nombres</strong><br>
            \(M_n\) est une variable aléatoire moyenne d'un échantillon de taille \(n\) de variables aléatoires
            indépendantes d'espérance \(\mu\) et de variance \(v\). On a :
            \(\displaystyle\lim_{n\to+\infty}P\left(\left|M_n-\mu\right|\geqslant\delta\right)=0\)
        </div>

        <div class="rema">
            Cette loi affirme que parmi tous les échantillons de valeurs possibles, ceux dont la moyenne s'éloigne de
            l'espérance sont rares, et que cette rareté s'accentue avec la taille de l'échantillon.
        </div>

        <div class="meth"><strong>Taille d'un échantillon</strong><br>
            Pour exploiter l'inégalité de concentration, on peut selon les cas agir sur \(n\) (agrandir l'échantillon)
            ou \(\delta \) (s'éloigner de l'éspérance), ou les deux, en les choississant assez grands pour que
            \(\frac{v}{n\delta^2}\) soit inférieur à une probabilité donnée.
        </div>
        <div class="exo">On lance \(n\) fois un dé en notant les résultats \(\left(X_i\right)_{1\leqslant i \leqslant
            n}\) inscrits sur la face supérieure pour chaque lancer.
            <ol>
                <li>
                    Calculer les espérances, variances et écarts-types de \(X_1\), de \(S_n\) et \(M_n\)
                    (\(\textrm{Var}(X)\approx2{,}917\)).
                </li>
                <li>
                    Appliquer l'inégalité de Bienaymé-Tchebychev à \(X_1\), pour \(\delta=2\), comparer le résultat
                    obtenu à un calcul direct de la probabilité.
                </li>
                <li>
                    Si \(n=10\), déterminer \(\delta \) pour que \(P\left(\left|M_n-\mu\right|\geqslant\delta\right)
                    \leqslant 5\% \)
                </li>
                <li>
                    Déterminer la taille de l'échantillon pour que la probabilité que \(M_n\) soit inférieure à 2 ou
                    supérieure à 5 soit inférieure à 1%.
                </li>
            </ol>
        </div>


        <h2> Approfondissements : Covariance</h2>
        <div class="defi">
            On nomme <strong>Covariance</strong> de \(X\) et de \(Y\) le nombre :
            \(\textrm{Cov}(X,Y) = \mathbb{E}\left(\left(X -\mathbb{E}(X)\right)\left(Y-\mathbb{E}(Y)\right)\right)\)<br>
            \(X\) et \(Y\) sont dites <strong>non corrélées</strong> lorsque leur covariance est nulle.
        </div>
        <div class="exo">
            <ol>
                <li>
                    Démontrer que \(\textrm{Cov}(X,Y) = \mathbb{E}(XY) -\mathbb{E}(X)\mathbb{E}(Y)\)
                </li>
                <li>
                    En développant, montrer que
                    \(\textrm{Var}(X+Y) = \textrm{Var}(X) + \textrm{Var}(Y) +2\textrm{Cov}(X,Y)
                    \)
                </li>
                <li>
                    En reprenant les variables aléatoires données à l'ex 2, montrer qu'elles sont non corrélées mais non
                    indépendantes.
                </li>
                <li>
                    Démontrer que \(\textrm{Cov}(X+Y,X-Y)=0\).
                </li>
                <li>
                    Le coefficient de corrélation linéaire est donné par
                    \(\textrm{r}(X,Y)=\frac{\textrm{Cov}(X,Y)}{\sigma(X)\sigma(Y)}\). <br>
                    En étudiant \(f(t)=\textrm{Var}(-tX+Y)\), démontrer que \(-1\leqslant r \leqslant 1\).
                </li>
                <li>
                    Démontrer que \(r=\pm 1\) équivaut à \(Y\) est fonction affine de \(X\) (indication : une loi de
                    variance nulle est ...).
                </li>
            </ol>
        </div>
        <div class="defi">
            La <strong>droite d'ajustement linéaire</strong> calculée par les outils numériques passe par le point moyen
            \(\left(\mathbb{E}(X) ; \mathbb{E}(Y)\right)\), et a pour coefficient directeur
            \(\dfrac{\textrm{Cov}(X,Y)}{\textrm{Var(X)}}\).
        </div>
        <h2>Approfondissement : Estimation</h2>
        <div class="none" style="font-size: smaller;">
            Les scientifiques et les outils numériques utilisent «deux» «écarts-types» : \(\sigma\) et \(s_n\).
            On dit que \(s_n\) est un estimateur «sans biais». Ce paragraphe permet d'éclaircir cette notion.
        </div>
        <div class="defi" style="font-size: smaller;">
            <ul>
                <li> On appelle <strong>estimateur</strong> sur un échantillon de taille n fonction (à valeurs
                    réelles)
                    de n
                    variables \(h(x_1,\ldots,x_n)\) ; <br>
                    par exemple \(h(x_1,\ldots,x_n)=\frac{1}{n}\left(x_1+\cdots+x_n\right)\) (formule de la
                    moyenne,
                    notée \(\overline{x}\) en statistiques), ou bien \(h(x_1,\ldots,x_n)=x_1\times x_2\) (peu
                    d'utilité).</li>
                <li> \(g(X_1,\ldots,X_n)\) étant une variable aléatoire, on peut noter \(\mathbb{E}(g)\), si
                    elle
                    existe, l'<strong>espérance</strong> de l'estimateur \(g\). </li>
                <li> Par essence, l'estimateur vise à approcher un paramètre de la population (par exemple la
                    moyenne du
                    caractère observé sur la population, une proportion, ...).<br>
                    Ainsi, lorsque \(k\) est la notation du paramètre approché, on <strong>peut noter</strong>
                    l'estimateur
                    \(\widehat{k}\) (avec un accent circonflexe), lorsqu'il n'y a pas d'ambigüité.</li>
                <li>Pour mesurer l'erreur entre l'estimation et la réalité, on utilise le <strong>biais</strong>
                    \(\mathbb{B}\),
                    défini par \(\mathbb{B}\left(\widehat{k}\right) = \mathbb{E}\left(\widehat{k}\right)-k\).
                    <br>
                    Lorsque son biais est nul, on dit que l'estimateur est <strong>sans biais</strong>.
                </li>
            </ul>
        </div>
        </div>
        <div class="prop"> L'estimateur \(\widehat{\overline{x}} = \frac{1}{n}\left(x_1+\cdots+x_n\right)\) est sans
            biais. <br>
            <strong>Démonstration : </strong> On note \(\mu\) la moyenne sur la population entière.<br>
            \(\mathbb{B}\left(\widehat{\overline{X}}\right)
            = \mathbb{E}\left(\frac{1}{n}\left(X_1+\cdots+X_n\right)\right)-\mu
            = \frac{1}{n}\left(\mathbb{E}\left((X_1\right)+\cdots+\mathbb{E}\left(X_n\right)\right)-\mu
            = \frac{1}{n}\left(\mu+\cdots+\mu\right)-\mu
            = \frac{1}{\cancel{n}}\cancel{n}\mu-\mu=0 \)
        </div>
        <div class="exo">
            On note \(\overline{X}\) la variable aléatoire définie par
            \(\overline{X}=\frac{1}{n}\left(X_1+\cdots+X_n\right)\) et on note \(V\) la variable aléatoire définie
            par
            \(V=\overline{X^2}-\overline{X}^2\).<br>
            On suppose que \(X_1,\ldots,X_n\) ont toutes pour espérance \(\mu\) et pour variance \(v\).<br>
            Démontrer que \(\mathbb{E}(V)=\dfrac{n-1}{n}v\) ; (utiliser
            \(\mathbb{E}(T^2)=\textrm{Var}(T)+\mathbb{E}(T)^2\)) ;<br>
            en déduire que le calcul de la variance sur l'échantillon ne fournit pas un estimateur sans biais de la
            variance sur la population ;<br>
            en déduire qu'un estimateur sans biais de la variance sur la population est donné par
            \(\dfrac{n}{n-1}V\).
        </div>
        <div class="defi"> <strong>Écart-type ponctuel (ou corrigé)</strong><br>
            On utilise l'<strong>écart-type ponctuel ou corrigé</strong> \(s_n=\sqrt{\frac{n}{n-1}}\sigma_n\) comme
            estimateur de
            l'écart-type sur la population globale ; \(\sigma_n\) étant l'écart-type calculé sur un échantillon de
            taille n.
        </div>



    </main>
</body>

</html>
